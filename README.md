# Certified Kubernetes Administrator(CKA)

## Useful Links

[Infrastructure for container projects - LXC, LXD & LXCFS](https://linuxcontainers.org/)

further reading on what are[LXD](https://help.ubuntu.com/lts/serverguide/lxd.html.en) and [LXC](https://help.ubuntu.com/lts/serverguide/lxc.html) container runtime technologies

[Components](https://kubernetes.io/docs/concepts/overview/components/)

[API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)

[Master-Node-Communication](https://kubernetes.io/docs/concepts/architecture/master-node-communication/)

[Storage](https://kubernetes.io/docs/concepts/storage/volumes/)

[Resources](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/)

## Basic Kubernetes Architecture

### Types of Nodes

- Master Node - could be server for HA setup (or) single node cluster
  - `kube-apiserver` - answers api call
  - `etcd` - key/value store used by API Server for configuration and other persistent storage needs
  - `kube-scheduler` - Determins which nodes are responsable for `PODS` and their respective containers as they are up in the cluster
  - `Cloud Controler Manager` - split out in to several containers depends on cloud platform we are running - responsible for persistatnt storage, routes for networking
  - `kube-controller-manager` - make all of the above avaialble to cloud through this service (or) we can block this if not needed

- Nodes - workers/minions are work force of the cluster
  - `kublet` - takes orders from master to run `PODS`
  - `kube-proxy` - assist with `Container Network Interface(CNI)` with routing traffic around the cluster
  - `POD` - one (or) more containers run as part of `POD` which are considered disposablea and replacable

## API Premitives (or) Cluster Objects

Persistent entities and represent state of the cluster like,

- What applications are running
- Which node those applications are running on
- Policies around those applications

K8s Objects are `records of intent`

`Object Spec`

- User provides desired state of an object
  
`Object Status`

- Provided by K8s to describe the actual state of the object which is managed by control plane of K8s

### Common K8s Objects

- `Nodes` - which makes the cluster
- `PODS` - single instance of application containers
- `Deployments` - LoadBalanced set of `PODS`
- `Services` - Exposes deployments to external networks by means of 3rd party LoadBalancer (or) ingress rules
- `ConfigMaps` - KV pair that can dynamically plugged in to other objects as needed which allows to decouple configuration from individual `PODS` (or) `Deployments` which gives lot of flexibility

### Names and UIDs

Names

- lower case alphanumeric with max len 253 characters and can be resued and provided by client but must be uniq. `BTW:` - and . are allowed.

UIDs

- generated by K8s

Namespaces

- multiple virtual clusters back by same physical cluster
- best for large deployments to isolate cluster resources and to define RBAC and quotas
- `kube-system` special namespace used to differenciate systems `PODS` from user `PODS`

Cloud Controller Manager

- `Route Controller(GCE clusters only)`
- `Service Controller` - responsbile for listening to service create/update/delete events and update cloud load balancers like AWS-ELB, Google-LB,Azure-LB to reflect the state of services in K8s
- `Persistent Volume Labels Controller` - applies labels AWS-EBS,GCE-PD(persistent disk) volumes when those get created which allows to manually define by user. These lables are centeral to scheduling of `PODS` as those labels are constrain to work only in the region/zone they are in.

Node Controller

- Assigns CIDR block to new nodes if assign CIDR is enabled
- Keeps track of the nodes using health of the node and changes the status of `Not Ready` to `UNKNOWN`when node becomes unreahable with-in the heartbeat intervals(`40 sec timeout`). If node state is flagged as UNKNOWN and `5 min` after that it evicts `PODS`(rate of `0.1/sec`\[effected by cluster_size(default 50) configurable by `large_cluster_size_threshold`\] means it will not effect `PODS` from more than `1 node every 10 sec`) from unhealthy nodes. `BTW:` evictions are stopped if nodes in all availablity zones are flagged as `UNKNOWN` because there could be scenario where `master node` could be having problem connecting to `worker nodes`
- Checks the status of nodes every few sec which is configurable by setting `node_monitor_period`

## Services & Network Primitives

- Servics refere to deployments and exposes particualr port via `kube-proxy` (or) using special IP Address for entire `POD` and prefernce of this setup strictly depends on network configuration, security policies, ingress rules to handle load balancing and port forwarding with-in each cluster deployment.

## Designing a K8s Cluster

### Local Deployment Options

- `Minikube` - single node K8s on local workstation
- `Kubeadm` - multi-nodes cluster on local workstation
- `Ubuntu on LXD` - provides nine-instance suppoted cluster on local workstation

### Cloud/COAS(Container Orchestrator as Service) Turnkey Solutions

- [conjure-up](https://docs.conjure-up.io/stable/en/) - deploy the Canonical Distribution of Kubernetes with Ubuntu on AWS, Azure, Google and Oracle Cloud
- AWS EKS, Azure AKS, Google GKE, Tectonic by CoreOS, Stackpoint.io, kube2go.io as well as madcore.ai

### Add-on Solutions

- Container Network Interface(CNI)
  - Calico - secured L3 networking and policy provider
  - Canal unites Flannel & Calico - provided networking and policies
  - Cilium - L3 network and policy plugin that can enforce HTTP/API/L7 policies transparently
  - CNI-Genie - enables K8s to seamlessly connect to CNI of any choice such as Calico, Canal, Flannel, Romana (or) Weave
  - NSX-T(NCP) provides integration between VMWare NSX-T and container orchestrator such as K8s as well as container-based platforms(CaaS,PaaS) like PCF and Openshift.
  - Nuage is a SDN platform that provides policy-based networking between K8s `PODS` and non-K8s environments with visibility and security monitoring
  - Weave Net provides networking and policy and doesn't require an external database.

## Hardware and Underlying Infrastructure

- Nodes including master can be physical/virtual running K8s components and a container runtime like Docker, rocket(rkt)
- nodes should be connected by a common network where `Flannel` is used as basic `PODS` networking application.

## Securing Cluster Communications

### Securing API

- Communication to API Server/Service, control-plane communications inside the cluster and even `POD-to-POD` communications.
- Default encryption communication is `TLS`
- Ensure any unsecure taffic/ports are `DISABLED`
- Anything that connects to API, including nodes, proxies, the scheduler, volume plugins and users should be `Authenticated`
- Certs for all internal communication channels are automatically creaded unless you chose to deploy [K8s cluster the hardway](https://github.com/kelseyhightower/kubernetes-the-hard-way)
- Once Authenticated `Authorization checks` should be passed.
- `RBAC Control Component` used to match users/groups to permission sets that are organized in to roles

### Securing Kubelet

- By default exposes HTTPS endpoints which gives access to both data and actions on the nodes, to enable Kubelet to use RBAC policies by starting it with a flag `--anonymous-auth=false` and assign an appropriate x509 client certificate in its configuration.

### Securing Network

- Network policies restrict access to network for a particualr namespace
- Users can be assigned quotas or limit ranges to prevent over usage of node ports (or) Load Balance services

### Vulnerabilities

- etcd - key/value store for entire cluster to store configuration and secrets of the cluster
- Isolate it behind FW so only it is accesses through API which is well enforced through RBAC
- Audit logging and archite the audit file to a secure server
- Rotate Infra Credentails frequently by setting short liftime and use any automated rotation methods supported by K8s

### Thrid Party Integrations

- Don't allow them in to kube-system namespace
- Always review integration before enabling

Join [K8s announcement group](https://groups.google.com/forum/#!forum/kubernetes-announce) for emails about security announcements.

## [High Availability(HA)](https://kubernetes.io/docs/setup/independent/high-availability/)

### Build Reliable Cluster

- Create reliable nodes that forms the cluster
- Setup redundant and reliable storage service with multinode deployment of `etcd`
- Start replicated and load balanced API Server/Service
- Setup a master-elected scheduler and controller manager daemons.

#### Step One - `Kubelet`

- Ensure cluster services automatically restart if they fail and `Kubelet`already does this, but what if `Kubelet` goes `DOWN`, so for that we need something like [monit](https://mmonit.com/monit/#slideshow) to watch the service/process.

#### Step Two - `Storage`

- Clustered `etcd` already replicates the storage to all master instances in the cluster
- Add additional reliability by increasing the cluster from 3 to 5 nodes and even more where applicable
- If using cloud provider then use any Block Device Persistent Storage offerings of that cloud provider to map/mount then on to your virtual machines in the cloud.
- If running on Baremetal - use options like NFS, Clustered Filesystem like Gluster (or) Ceph, Hardware level RAID

#### Step Three - `Replicated API Services`

- Refere to image in `images/k8s-ha-step3-1.png` and `images/k8s-ha-step3-2.png` files for detail steps

#### Step Four - Controller/Scheduler Daemons

- Allow our state to change, we use lease-lock on API to perform master election. So each scheduler and controller manager will be started using `--leader-elect` flag to ensure only 1 instance of scheduler and controller manager(CM) are running and making changes to cluster
- Scheduler and CM can be configured to talk to the API Server/Service that is on the same node (or) through Loadbalancd IP(preferred).

### Installing Configuration Files

- Create empty log files on each node, so that Docker will mount the files and not make new directhries,

  ```bash
  touch /var/log/kube-scheduler.log
  touch /var/log/kube-controller-manager.log
  ```

- Setup the description of the scheduler and CM `PODS`on each node by copying the `kube-scheduler.yaml` and `kube-controller-manager.yaml` in to the `/etc/kubernetes/manifests/` directory.

## End-to-End Testing & Validation Nodes & the Cluster

- `Kubetest Suite` - ideal of GCE and AWS users to,
  - Build
  - Stage
  - Extract
  - Bring up the Cluster
  - Test
  - Dump logs
  - Tear Down the cluster.

- manual validation
  - `kubectl get nodes`
  - `kubectl desribe node <node name>`
  - `kubectl get pods --all-namespaces -o wide`
  - `kubectl get pods -n kube-system -o wide`
  - `ps aux | grep [k]ube` - validate with kube processes are running.

## [Labels & Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

- Labels are key/value pairs that are attached to objects, such as pods
- Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users
- One use case of labels is to [constrain the set of nodes onto which a pod can schedule](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
- The API currently supports two types of selectors: [equality-based](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#equality-based-requirement) and [set-based](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement)
  - `equality-based`

  ```bash
  kubectl get pods -l environment=production,tier=frontend
  ```

  - `set-based`
    - Newer resources, such as Job, Deployment, Replica Set, and Daemon Set support set-based requirements as well.

  ```bash
  kubectl get pods -l 'environment in (production),tier in (frontend)'
  ```